{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Evaluation**","metadata":{"id":"LkDu9civ3Wcx"}},{"cell_type":"markdown","source":"**Intrinsic-Word similarity**","metadata":{"id":"AW1eLbxC3enE"}},{"cell_type":"markdown","source":"1. Import dependancies","metadata":{"id":"1T2ffVMR3yZw"}},{"cell_type":"code","source":"import numpy as np\nimport scipy\nfrom scipy.stats import *\nimport sys","metadata":{"id":"khiamwbT3bzG","execution":{"iopub.status.busy":"2021-06-19T21:02:49.429093Z","iopub.execute_input":"2021-06-19T21:02:49.429868Z","iopub.status.idle":"2021-06-19T21:02:49.434607Z","shell.execute_reply.started":"2021-06-19T21:02:49.429823Z","shell.execute_reply":"2021-06-19T21:02:49.433671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. define data-load and calculation functions ","metadata":{"id":"PdvaA_hz4OYp"}},{"cell_type":"code","source":"embeddings = \"\"\n\n\ndef loadData():\n    ret = {}\n    # I use Colab here so I simply copy the data's path, I'll upload all the data this notebook need, if the path doesn't work, please change it manualy.\n    #I comment the other spine embeddings, while evaluate other word embeddings please remove the \"#\" sign and comment present evaluated word embeddings.\n    data = open(\"../input/spine-embeddings/glove.6B.300d.txt.spine\",\"r\").readlines()#glove\n    #data = open(\"../input/spine-embeddings/GoogleNews-vectors-negative300(first500000).txt.spine\",\"r\").readlines()#Word2Vec\n    #data = open(\"../input/spine-embeddings/wiki-news-300d-1M.txt.spine\",\"r\").readlines()#Fasttext\n    #data = open(\"../input/spine-embeddings/sentence_embeddings.txt.spine\",\"r\").readlines()#sentence embedding\n    #data = open(\"../input/spine-embeddings/image_embeddings.txt.spine\",\"r\").readlines()#image embedding\n    \n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.15_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.15, batch size = 64, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.01_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.01, batch size = 64, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.001_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.001, batch size = 64, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.0001_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.0001, batch size = 64, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_16_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 16, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_32_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 32, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_128_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 128, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_256_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 256, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.1.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.1\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.3.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.3\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.6.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.6\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.9.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.9\n    \n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_without_psl.spine\",\"r\").readlines()#without psl loss\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_without_asl.spine\",\"r\").readlines()#without asl loss\n    for row in data:\n        word = row.strip().split(' ')[0]\n        vals = row.strip().split(' ')[1:]\n        vals = np.array( [float(val) for val in vals] )\n        ret[word] = vals\n    return ret\n\n\ndef loadTestData():\n    # I use Colab here so I simply copy the data's path, I'll upload all the data this notebook need, if the path doesn't work, please change it manualy.\n    tmp = open(\"../input/word-sim/word_sim.tab\",\"r\").readlines()\n    data = {}\n    data['words'] = [ row.strip().split('\\t')[0:2] for i,row in enumerate(tmp) if i!=0 ]\n    data['sim_scores'] = [ float(row.strip().split('\\t')[2]) for i,row in enumerate(tmp) if i!=0  ]\n    return data\n\ndef getSimilarity(e1, e2):\n    # cosine similarity\n    return np.sum(e1 * e2)/( np.sqrt(np.sum(e1*e1)) * np.sqrt(np.sum(e2*e2)))\n\ndef getSimilarityScoreForWords(w1,w2):\n\tglobal embeddings\n\t#print w1\n\t#print embeddings\n\tif (w2 not in embeddings) or (w1 not in embeddings) :\n\t\treturn -1\n\tfinalVector_w1 = embeddings[w1]\n\tfinalVector_w2 = embeddings[w2]\n\treturn getSimilarity(finalVector_w1, finalVector_w2)\n\ndef evaluate():\n\tglobal embeddings\n\tprint(\"--- loading data...\")\n\tembeddings = loadData()\n\tdata = loadTestData()\n\tprint(\"#words = \", len(data['words']))\n\tprint(\"#scores = \", len(data['sim_scores']))\n\tprint(\"--- checking...\")\n\tpred_scores = []\n\tinvalid = 0\n\tpred_scores = [ [getSimilarityScoreForWords(w1w2[0],w1w2[1]),human_score] for w1w2,human_score in zip(data['words'], data['sim_scores']) ]\t\t\t\n\tpred_scores = np.array( [ val for val in pred_scores if val[0]!=-1 ] )\n\t#print pred_scores\n\tspearman_rank_coeff,sp_rho = spearmanr( pred_scores[:,0], pred_scores[:,1] )\n\tprint(\"total, valid,spearman_rank_coeff,sp_rho \", len(data['words']),len(pred_scores), spearman_rank_coeff,sp_rho)\n","metadata":{"id":"2BgEcbrp4N3_","execution":{"iopub.status.busy":"2021-06-19T21:02:50.972474Z","iopub.execute_input":"2021-06-19T21:02:50.973308Z","iopub.status.idle":"2021-06-19T21:02:51.098792Z","shell.execute_reply.started":"2021-06-19T21:02:50.973248Z","shell.execute_reply":"2021-06-19T21:02:51.097726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T21:02:53.698399Z","iopub.execute_input":"2021-06-19T21:02:53.699117Z","iopub.status.idle":"2021-06-19T21:05:44.514192Z","shell.execute_reply.started":"2021-06-19T21:02:53.699075Z","shell.execute_reply":"2021-06-19T21:05:44.512717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**extrinsic_downstream**","metadata":{}},{"cell_type":"markdown","source":"*newsgroups*","metadata":{}},{"cell_type":"markdown","source":"1. get evaluate data","metadata":{}},{"cell_type":"code","source":"from nltk import word_tokenize\nfrom sklearn.datasets import fetch_20newsgroups\nimport pickle\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\nimport nltk\nnltk.download('punkt')\n\ncomputer_categories = ['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\nreligion_categories = ['alt.atheism', 'soc.religion.christian']\n\ndef get_Xy(data):\n    X, y = [], []\n    for idx in range(len(data['data'])):\n        X.append(word_tokenize(data['data'][idx].lower()))\n        y.append(data['target'][idx])\n    return X, y\n\n# I use Colab here so I simply copy the data's path, I'll upload all the data this notebook need, if the path doesn't work, please change it manualy.\ndef get_everything(categories, val_fraction, name, data_dir=\"./\"):\n    data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n    data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n    data_test_X, data_test_y = get_Xy(data_test)\n    data_X, data_y = get_Xy(data_train)\n    \n    print(\"len(data_X):\",len(data_X))\n    print(\"len(data_test_X):\",len(data_test_X))\n    val_threshold = int( (1.0 - val_fraction)* len(data_X) )\n\n    data_train_X, data_train_y = data_X[:val_threshold], data_y[:val_threshold]\n    data_val_X, data_val_y = data_X[val_threshold:], data_y[val_threshold:]\n    \n    pickle.dump(data_train_X, open(data_dir+name+\"_train_X.p\", 'wb'))\n    pickle.dump(data_train_y, open(data_dir+name+\"_train_y.p\", 'wb'))\n    pickle.dump(data_val_X, open(data_dir+name+\"_val_X.p\", 'wb'))\n    pickle.dump(data_val_y, open(data_dir+name+\"_val_y.p\", 'wb'))\n    pickle.dump(data_test_X, open(data_dir+name+\"_test_X.p\", 'wb'))\n    pickle.dump(data_test_y, open(data_dir+name+\"_test_y.p\", 'wb'))\n\n    \nget_everything(computer_categories,0.2,'news_computer')\nget_everything(religion_categories,0.2,'news_religion')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T17:41:14.092872Z","iopub.execute_input":"2021-06-21T17:41:14.093442Z","iopub.status.idle":"2021-06-21T17:41:37.762006Z","shell.execute_reply.started":"2021-06-21T17:41:14.093351Z","shell.execute_reply":"2021-06-21T17:41:37.760731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. evaluate","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n#from sklearn.model_selection import train_test_split\n#from sklearn.preprocessing import StandardScaler\n#from sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\n#from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n#from sklearn.gaussian_process import GaussianProcessClassifier\n#from sklearn.gaussian_process.kernels import RBF\n#from sklearn.tree import DecisionTreeClassifier\n#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\n#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nimport sys\nimport pickle\n\nh = .02  # step size in the mesh\nembedding_size = None\nvectors = None\n\n# python classify.py embedding_src num_classes x_train_pickle y_train_pickle x_val_pickle y_val_pickle x_test_pickle y_test_pickle\n\nbest_val = 0.0\nbest_test = 0.0\n\ndef loadVectors():\n    global embedding_size\n    global vectors\n    # I use Colab here so I simply copy the data's path, I'll upload all the data this notebook need, if the path doesn't work, please change it manualy.\n    #I comment the other spine embeddings, while evaluate other word embeddings please remove the \"#\" sign and comment present evaluated word embeddings.\n    data = open(\"../input/spine-embeddings/glove.6B.300d.txt.spine\",\"r\").readlines()#glove\n    #data = open(\"../input/spine-embeddings/GoogleNews-vectors-negative300(first500000).txt.spine\",\"r\").readlines()#Word2Vec\n    #data = open(\"../input/spine-embeddings/wiki-news-300d-1M.txt.spine\",\"r\").readlines()#Fasttext\n    #data = open(\"../input/spine-embeddings/sentence_embeddings.txt.spine\",\"r\").readlines()#sentence embedding\n    #data = open(\"../input/spine-embeddings/image_embeddings.txt.spine\",\"r\").readlines()#image embedding\n    \n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.15_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.15, batch size = 64, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.01_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.01, batch size = 64, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.001_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.001, batch size = 64, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.0001_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.0001, batch size = 64, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_16_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 16, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_32_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 32, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_128_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 128, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_256_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 256, momentum = 0\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.1.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.1\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.3.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.3\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.6.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.6\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.9.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.9\n    \n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_without_psl.spine\",\"r\").readlines()#without psl loss\n    #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_without_asl.spine\",\"r\").readlines()#without asl loss\n    vectors = {}\n    for row in data:\n        vals = row.split()\n        word = vals[0]\n        vals = np.array( [float(val) for val in vals[1:]] )\n        vectors[word] = vals\n    embedding_size = len(vals)\n    print(\"embedding_size = \",embedding_size)\n    return vectors\n\n\ndef getFeats(sentence):\n    global vectors\n    global embedding_size\n    ret = np.zeros(embedding_size)\n    cnt = 0\n    for word in sentence:\n        if word.lower() in vectors:\n            ret+=vectors[word.lower()]\n            cnt+=1\n    if cnt>0:\n        ret/=cnt\n    return ret\n\ndef getOneHot(vals, max_num):\n    ret = np.zeros((vals.shape[0], max_num))\n    for i,val in enumerate(vals):\n        ret[i][val] = 1\n    return ret\n\ndef trainAndTest(x_splits, y_splits, clf):\n    global best_val, best_test\n    # print(\"x_splits[0] = \", x_splits[0])\n    # print(\"x_splits[0] = \", type(x_splits[0]))\n    clf.fit(x_splits[0], y_splits[0])\n    flag = False\n    if len(x_splits[1])>0:\n        score = clf.score(x_splits[1], y_splits[1])\n        if score > best_val:\n            flag = True\n            best_val = score\n        print(\"Val Score = \", score)\n    score = clf.score(x_splits[2], y_splits[2])\n    if flag:\n        best_test = score\n    print(\"Test Score = \", score)\n\ndef main():\n    loadVectors()\n    num_classes = 2\n    print(\"num_classes = \",num_classes)\n    classifiers = None\n    if num_classes==2:\n        classifiers = [\n        SVC(kernel=\"linear\", C=0.025, class_weight='balanced'),\n        SVC(kernel=\"linear\", C=0.1, class_weight='balanced'),\n        SVC(kernel=\"linear\", C=5, class_weight='balanced'),\n        SVC(kernel=\"linear\", C=10, class_weight='balanced'),\n        SVC(kernel=\"linear\", C=50, class_weight='balanced'),\n        SVC(kernel=\"linear\", C=100, class_weight='balanced'),\n        SVC(kernel=\"linear\", C=500, class_weight='balanced'),\n        SVC(kernel=\"linear\", C=1000, class_weight='balanced'),\n        SVC(kernel=\"linear\", C=0.25, class_weight='balanced'),\n        SVC(gamma=2, C=0.1, class_weight='balanced'),\n        SVC(gamma=2, C=0.25, class_weight='balanced'),\n        SVC(C=0.1, class_weight='balanced'),\n        SVC(C=5, class_weight='balanced'),\n        SVC(C=10, class_weight='balanced'),\n        SVC(C=50, class_weight='balanced'),\n        SVC(C=100, class_weight='balanced'),\n        SVC(C=500, class_weight='balanced'),\n        SVC(C=1000, class_weight='balanced'),\n        SVC(class_weight='balanced'),\n        MLPClassifier(alpha=1),\n        GaussianNB(),\n        RandomForestClassifier()]\n    else:\n        #TODO\n        pass\n\n\n    all_feats = []\n    labels = []\n    idx = 3\n    # I use Colab here so I simply copy the data's path, I'll upload all the data this notebook need, if the path doesn't work, please change it manualy.\n    while idx<8:\n        if idx == 3:\n            texts = pickle.load( open(\"./news_computer_train_X.p\",\"rb\") )\n        if idx == 5:\n            texts = pickle.load( open(\"./news_computer_test_X.p\",\"rb\") )\n        if idx == 7:\n            texts = pickle.load( open(\"./news_computer_val_X.p\",\"rb\") )\n        if len(texts)>0:\n            feats = np.array( [getFeats(t) for t in texts] )\n            print(\"feats : \",feats.shape)\n            all_feats.append( feats )\n            idx+=1\n            if idx == 4:\n                cur_labels = np.array(pickle.load( open(\"./news_computer_train_y.p\",\"rb\") ) )\n            if idx == 6:\n                cur_labels = np.array(pickle.load( open(\"./news_computer_test_y.p\",\"rb\") ) )\n            if idx == 8:\n                cur_labels = np.array(pickle.load( open(\"./news_computer_val_y.p\",\"rb\") ) )\n                \n            #cur_labels = np.array(pickle.load( open(sys.argv[idx],\"rb\") ) )\n            #cur_labels = getOneHot(cur_labels, max(cur_labels)+1)\n            labels.append( cur_labels )\n            print(\"cur_labels : \",cur_labels.shape)\n            idx+=1\n        else:\n            idx+=2\n            labels.append([])\n            all_feats.append([])\n\n    for clf in classifiers:\n        #print \"=\"*33\n        #print \"clf = \",clf\n        trainAndTest(all_feats, labels, clf)\n\nprint(\"on going\")\nmain()\nprint('best val', best_val)\nprint('best test', best_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T17:55:02.939680Z","iopub.execute_input":"2021-06-21T17:55:02.940078Z","iopub.status.idle":"2021-06-21T17:58:23.860897Z","shell.execute_reply.started":"2021-06-21T17:55:02.940046Z","shell.execute_reply":"2021-06-21T17:58:23.860137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*np-bracketing*","metadata":{}},{"cell_type":"markdown","source":"1. get evaluate data ","metadata":{}},{"cell_type":"code","source":"!wget http://angelikilazaridou.github.io/resourses/parsing/NP_dataset.tar.gz\n!tar -xvzf NP_dataset.tar.gz","metadata":{"execution":{"iopub.status.busy":"2021-06-21T18:01:47.799923Z","iopub.execute_input":"2021-06-21T18:01:47.800286Z","iopub.status.idle":"2021-06-21T18:01:50.039793Z","shell.execute_reply.started":"2021-06-21T18:01:47.800238Z","shell.execute_reply":"2021-06-21T18:01:50.038697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport numpy\ndata = open(\"dataset.txt\",\"r\").readlines()[1:]\ndata_dir = \"./\"\nname = \"npbracketing\"\n\nx_splits = {i:[] for i in range(10)}\ny_splits = {i:[] for i in range(10)}\ntarget_dict = {'R':0,'L':1}\nfor row in data:\n    vals = row.strip().split()\n    split = int(vals[0])\n    target = target_dict[vals[-1]]\n    x_splits[split].append( [word_pos.split('-')[0] for word_pos in vals[1].split('_') ] )\n    y_splits[split].append(target)\n\nfor i in range(10):\n    print(len(x_splits[i]), x_splits[i][0])\n    print(len(y_splits[i]), y_splits[i][0])\n    x_train = x_splits[i]\n    y_train = y_splits[i]\n    x_val = []\n    y_val = []\n    x_test = []\n    y_test = []\n    for j in range(10):\n        if j!=i:\n            x_test.extend(x_splits[j])\n            y_test.extend(y_splits[j])\n    pickle.dump(x_train,open(data_dir+name+\"_\"+\"train_X\"+str(i)+\".pickle\",\"wb\"))\n    pickle.dump(x_val,open(data_dir+name+\"_\"+\"val_X\" + str(i) + \".pickle\",\"wb\"))\n    pickle.dump(x_test,open(data_dir+name+\"_\"+\"test_X\" + str(i) + \".pickle\",\"wb\"))\n    pickle.dump(y_train,open(data_dir+name+\"_\"+\"train_y\"+ str(i) + \".pickle\",\"wb\"))\n    pickle.dump(y_val,open(data_dir+name+\"_\"+\"val_y\"+ str(i) + \".pickle\",\"wb\"))\n    pickle.dump(y_test,open(data_dir+name+\"_\"+\"test_y\" + str(i) + \".pickle\",\"wb\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T18:02:40.363643Z","iopub.execute_input":"2021-06-21T18:02:40.364033Z","iopub.status.idle":"2021-06-21T18:02:40.423837Z","shell.execute_reply.started":"2021-06-21T18:02:40.363998Z","shell.execute_reply":"2021-06-21T18:02:40.422829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(2):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.colors import ListedColormap\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.datasets import make_moons, make_circles, make_classification\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm import SVC\n    from sklearn.gaussian_process import GaussianProcessClassifier\n    from sklearn.gaussian_process.kernels import RBF\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    from sklearn.ensemble import RandomForestClassifier\n    import sys\n    import pickle\n\n    h = .02  # step size in the mesh\n    embedding_size = None\n    vectors = None\n\n    # python classify.py embedding_src num_classes x_train_pickle y_train_pickle x_val_pickle y_val_pickle x_test_pickle y_test_pickle\n\n    def loadVectors():\n        global embedding_size\n        global vectors\n        # I use Colab here so I simply copy the data's path, I'll upload all the data this notebook need, if the path doesn't work, please change it manualy.\n        #I comment the other spine embeddings, while evaluate other word embeddings please remove the \"#\" sign and comment present evaluated word embeddings.\n        data = open(\"../input/spine-embeddings/glove.6B.300d.txt.spine\",\"r\").readlines()#glove\n        #data = open(\"../input/spine-embeddings/GoogleNews-vectors-negative300(first500000).txt.spine\",\"r\").readlines()#Word2Vec\n        #data = open(\"../input/spine-embeddings/wiki-news-300d-1M.txt.spine\",\"r\").readlines()#Fasttext\n        #data = open(\"../input/spine-embeddings/sentence_embeddings.txt.spine\",\"r\").readlines()#sentence embedding\n        #data = open(\"../input/spine-embeddings/image_embeddings.txt.spine\",\"r\").readlines()#image embedding\n\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.15_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.15, batch size = 64, momentum = 0\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.01_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.01, batch size = 64, momentum = 0\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.001_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.001, batch size = 64, momentum = 0\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.0001_bs_64_mm_0.spine\",\"r\").readlines()#learning rate = 0.0001, batch size = 64, momentum = 0\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_16_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 16, momentum = 0\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_32_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 32, momentum = 0\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_128_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 128, momentum = 0\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_256_mm_0.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 256, momentum = 0\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.1.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.1\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.3.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.3\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.6.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.6\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_lr0.1_bs_64_mm_0.9.spine\",\"r\").readlines()#learning rate = 0.1, batch size = 64, momentum = 0.9\n\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_without_psl.spine\",\"r\").readlines()#without psl loss\n        #data = open(\"../input/spine-embeddings/glove.6B.300d.txt_without_asl.spine\",\"r\").readlines()#without asl loss\n        vectors = {}\n        for row in data:\n            vals = row.split()\n            word = vals[0]\n            vals = np.array( [float(val) for val in vals[1:]] )\n            vectors[word] = vals\n        embedding_size = len(vals)\n        #print \"embedding_size = \",embedding_size\n        return vectors\n\n\n    def getFeats(sentence):\n        global vectors\n        global embedding_size\n        if False:\n            ret = np.zeros(embedding_size)\n            cnt = 0\n            for word in sentence:\n                if word in vectors:\n                    ret+=vectors[word]\n                    cnt+=1\n            if cnt>0:\n                ret/=cnt\n            return ret\n        else:\n            ret = []\n            cnt = 0\n            for word in sentence:\n                if word in vectors:\n                    ret.extend([ v for v in vectors[word]])\n                    cnt+=1\n                else:\n                    ret.extend( [v for v in np.zeros(embedding_size)] )\n            ret = np.array(ret)\n            return ret\n\n    def getOneHot(vals, max_num):\n        ret = np.zeros((vals.shape[0], max_num))\n        for i,val in enumerate(vals):\n            ret[i][val] = 1\n        return ret\n\n    def trainAndTest(x_splits, y_splits, clf):\n        clf.fit(x_splits[0], y_splits[0])\n        train_score = clf.score(x_splits[0], y_splits[0])\n        val_score = None\n        if len(x_splits[1])>0:\n            val_score = clf.score(x_splits[1], y_splits[1])\n            #print \"Val Score = \", val_score\n        score = clf.score(x_splits[2], y_splits[2])\n        #print \"Test Score = \", score\n        return train_score,val_score,score\n\n    def main():\n        loadVectors()\n        num_classes = 2\n        #print \"num_classes = \",num_classes\n        classifiers = None\n        if num_classes==2:\n            classifiers = [\n            SVC(kernel=\"linear\", C=0.025),\n            SVC(kernel=\"linear\", C=0.1),\n            SVC(kernel=\"linear\", C=1.0),\n            SVC(gamma=2, C=1),\n            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n            RandomForestClassifier(max_depth=5, n_estimators=50, max_features=10),\n            MLPClassifier(alpha=1),\n            RandomForestClassifier(n_estimators=20, max_features=10)]\n        else:\n            #TODO\n            pass\n\n\n        all_feats = []\n        labels = []\n        idx = 3\n        while idx<8:\n            if i == 0 and idx == 3:\n                texts = pickle.load( open(\"./npbracketing_train_X0.pickle\",\"rb\") )\n            if i == 0 and idx == 5:\n                texts = pickle.load( open(\"./npbracketing_val_X0.pickle\",\"rb\") )\n            if i == 0 and idx == 7:\n                texts = pickle.load( open(\"./npbracketing_test_X0.pickle\",\"rb\") )\n            if i == 1 and idx == 3:\n                texts = pickle.load( open(\"./npbracketing_train_X1.pickle\",\"rb\") )\n            if i == 1 and idx == 5:\n                texts = pickle.load( open(\"./npbracketing_val_X1.pickle\",\"rb\") )\n            if i == 1 and idx == 7:\n                texts = pickle.load( open(\"./npbracketing_test_X1.pickle\",\"rb\") )\n            #texts = pickle.load( open(sys.argv[idx],\"rb\") )\n            if len(texts)>0:\n                feats = np.array( [getFeats(t) for t in texts] )\n                #print \"feats : \",feats.shape\n                all_feats.append( feats )\n                idx+=1\n                if i == 0 and idx == 4:\n                    cur_labels = pickle.load( open(\"./npbracketing_train_y0.pickle\",\"rb\") )\n                if i == 0 and idx == 6:\n                    cur_labels = pickle.load( open(\"./npbracketing_val_y0.pickle\",\"rb\") )\n                if i == 0 and idx == 8:\n                    cur_labels = pickle.load( open(\"./npbracketing_test_y0.pickle\",\"rb\") )\n                if i == 1 and idx == 4:\n                    cur_labels = pickle.load( open(\"./npbracketing_train_y1.pickle\",\"rb\") )\n                if i == 1 and idx == 6:\n                    cur_labels = pickle.load( open(\"./npbracketing_val_y1.pickle\",\"rb\") )\n                if i == 1 and idx == 8:\n                    cur_labels = pickle.load( open(\"./npbracketing_test_y1.pickle\",\"rb\") )\n                #cur_labels = np.array(pickle.load( open(sys.argv[idx],\"rb\") ) )\n                #cur_labels = getOneHot(cur_labels, max(cur_labels)+1)\n                labels.append( cur_labels )\n                #print \"cur_labels : \",cur_labels.shape\n                idx+=1\n            else:\n                idx+=2\n                labels.append([])\n                all_feats.append([])\n        print(\"Done loading data\")\n\n        best_test = 0.0\n        best_clf = None\n        best = 0.0\n        for clf in classifiers:\n            #print \"=\"*33\n            #print \"clf = \",clf\n            score, val_score, test_score = trainAndTest(all_feats, labels, clf)\n            best_test = max(best_test, test_score)\n            if score>best:\n                best = score\n                best_clf = clf\n        print(\"best_test for this split= \", best_test)\n        #print \"best_test = \", best_test\n        #print \"best= \", best, best_clf\n\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T18:21:35.068776Z","iopub.execute_input":"2021-06-21T18:21:35.069296Z","iopub.status.idle":"2021-06-21T18:26:50.897188Z","shell.execute_reply.started":"2021-06-21T18:21:35.069225Z","shell.execute_reply":"2021-06-21T18:26:50.896045Z"},"trusted":true},"execution_count":null,"outputs":[]}]}