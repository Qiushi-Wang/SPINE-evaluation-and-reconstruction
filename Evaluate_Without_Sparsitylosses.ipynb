{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. Import dependancies","metadata":{"id":"YL9rJiMVzOmb"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport logging\n\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nimport torch\n#from torch import nn\nimport argparse","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-14T15:35:06.978548Z","iopub.execute_input":"2021-06-14T15:35:06.978969Z","iopub.status.idle":"2021-06-14T15:35:06.983869Z","shell.execute_reply.started":"2021-06-14T15:35:06.978935Z","shell.execute_reply":"2021-06-14T15:35:06.982629Z"},"id":"z3z2fQ7JvNjy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. establisch model(without psl loss)","metadata":{"id":"WiryvWz4vNjz"}},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO)\n\nclass SPINEModel(torch.nn.Module):\n    def __init__(self, params):\n        super(SPINEModel, self).__init__()\n        # params\n        self.inp_dim = params['inp_dim']\n        self.hdim = params['hdim']\n        self.noise_level = params['noise_level']\n        self.getReconstructionLoss = nn.MSELoss()\n        self.rho_star = 1.0 - params['sparsity']\n        \n        # autoencoder\n        logging.info(\"Building model \")\n        self.linear1 = nn.Linear(self.inp_dim, self.hdim)\n        self.linear2 = nn.Linear(self.hdim, self.inp_dim)\n        \n\n    def forward(self, batch_x, batch_y):\n        # forward\n        batch_size = batch_x.size(0)\n        linear1_out = self.linear1(batch_x)\n        h = linear1_out.clamp(min=0, max=1) # capped relu\n        out = self.linear2(h)\n\n        # different terms of the loss\n        reconstruction_loss = self.getReconstructionLoss(out, batch_y) # reconstruction loss\n        #psl_loss = self._getPSLLoss(h, batch_size) \t\t# partial sparsity loss\n        asl_loss = self._getASLLoss(h)    \t# average sparsity loss\n        total_loss = reconstruction_loss + asl_loss\n        \n        return out, h, total_loss, [reconstruction_loss, asl_loss]\n\n\n    def _getPSLLoss(self,h, batch_size):\n        return torch.sum(h*(1-h))/ (batch_size * self.hdim)\n\n\n    def _getASLLoss(self, h):\n        temp = torch.mean(h, dim=0) - self.rho_star\n        temp = temp.clamp(min=0)\n        return torch.sum(temp * temp) / self.hdim","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:07.023756Z","iopub.execute_input":"2021-06-14T15:35:07.024227Z","iopub.status.idle":"2021-06-14T15:35:07.055622Z","shell.execute_reply.started":"2021-06-14T15:35:07.024184Z","shell.execute_reply":"2021-06-14T15:35:07.054687Z"},"id":"KocJnAayvNjz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. data load function","metadata":{"id":"pv-bBBdgvNj0"}},{"cell_type":"code","source":"logging.basicConfig(level=logging.DEBUG)\n\nclass DataHandler:\n    \n    def __init__(self):\n        pass\n\n    \n    def loadData(self, filename):\n        \n        #limit = 1000 ## for debnugging. TODO: remove this\n        lines = open(filename).readlines()#[:limit]\n        self.data = []\n        self.words = []\n        for line in lines:\n\t\t\ttokens = line.strip().split()\n\t\t\tif len(tokens[1:]) != 300 and len(tokens[1:]) != 1280:\n\t\t\t\tline = ''\n\t\t\tfor token in tokens:\n\t\t\t\tif token == \"nan\":\n\t\t\t\t\tline = ''\n\t\t\tif line == '':\n\t\t\t\tcontinue\n\t\t\ttokens = line.strip().split()\n\t\t\tself.words.append(tokens[0])\n\t\t\tself.data.append([float(i) for i in tokens[1:]])\n        \n        self.data = np.array(self.data)\n        logging.info(\"Loaded data. #shape = \" + str(self.data.shape))\n        logging.info(\" #words = %d \" %(len(self.words)) )\n        self.data_size = self.data.shape[0]\n        self.inp_dim = self.data.shape[1]\n        self.original_data = self.data[:]\n        logging.debug(\"original_data[0][0:5] = \" + str(self.original_data[0][0:5]))\n\n\n    def getWordsList(self):\n        return self.words\n\n    def getDataShape(self):\n        return self.data.shape\n\n    def resetDataOrder(self):\n        self.data = self.original_data[:]\n        logging.debug(\"original_data[0][0:5] = \" + str(self.original_data[0][0:5]))\n\n    def getNumberOfBatches(self, batch_size):\n        return int(( self.data_size + batch_size - 1 ) / batch_size)\n\n    def getBatch(self, i, batch_size, noise_level, denoising):\n        batch_y = self.data[i*batch_size:min((i+1)*batch_size, self.data_size)]\n        batch_x = batch_y\n        if denoising:\n            batch_x = batch_y + get_noise_features(batch_y.shape[0], self.inp_dim, noise_level)\n        return batch_x, batch_y\n\n    def shuffleTrain(self):\n        indices = np.arange(self.data_size)\n        np.random.shuffle(indices)\n        self.data = self.data[indices]","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:07.056996Z","iopub.execute_input":"2021-06-14T15:35:07.057487Z","iopub.status.idle":"2021-06-14T15:35:07.070908Z","shell.execute_reply.started":"2021-06-14T15:35:07.057443Z","shell.execute_reply":"2021-06-14T15:35:07.070031Z"},"id":"_xz5guCivNj1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_sparsity(X):\n    non_zeros = 1. * np.count_nonzero(X)\n    total = X.size\n    sparsity = 100. * (1 - (non_zeros)/total)\n    return sparsity\n\ndef dump_vectors(X, outfile, words):\n    print (\"shape\", X.shape)\n    assert len(X) == len(words) #TODO print error statement\n    fw = open(outfile, 'w')\n    for i in range(len(words)):\n        fw.write(words[i] + \" \")\n        for j in X[i]:\n            fw.write(str(j) + \" \")\n        fw.write(\"\\n\")\n    fw.close()\n\ndef get_noise_features(n_samples, n_features, noise_amount):\n    noise_x,  _ =  make_blobs(n_samples=n_samples, n_features=n_features, \n                cluster_std=noise_amount,\n                centers=np.array([np.zeros(n_features)]))\n    return noise_x","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:07.072511Z","iopub.execute_input":"2021-06-14T15:35:07.072964Z","iopub.status.idle":"2021-06-14T15:35:07.088139Z","shell.execute_reply.started":"2021-06-14T15:35:07.072923Z","shell.execute_reply":"2021-06-14T15:35:07.087061Z"},"id":"UkFKije3vNj1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. define train and get embeddings function(without psl loss)","metadata":{"id":"56qGyRryvNj2"}},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO)\n\nclass Solver:\n\n\tdef __init__(self, params):\n\n\t\t# Build data handler\n\t\tself.data_handler = DataHandler()\n\t\tself.data_handler.loadData(params['input'])\n\t\tparams['inp_dim'] = self.data_handler.getDataShape()[1]\n\t\tlogging.info(\"=\"*41)\n\n\n\t\t# Build model\n\t\tself.model = SPINEModel(params)\n\t\tself.dtype = torch.FloatTensor\n\t\tuse_cuda = torch.cuda.is_available()\n\t\tif use_cuda:\n\t\t\tself.model.cuda()\n\t\t\tself.dtype = torch.cuda.FloatTensor\n\t\tself.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.1)\n\t\tlogging.info(\"=\"*41)\n\n\n\tdef train(self, params):\n\t\tnum_epochs, batch_size = params['num_epochs'], params['batch_size'],\n\t\toptimizer = self.optimizer\n\t\tdtype = self.dtype\n\t\tfor iteration in range(num_epochs):\n\t\t\tself.data_handler.shuffleTrain()\n\t\t\tnum_batches = self.data_handler.getNumberOfBatches(batch_size)\n\t\t\tepoch_losses = np.zeros(4) # rl, asl, psl, total\n\t\t\tfor batch_idx in range(num_batches):\n\t\t\t\toptimizer.zero_grad()\n\t\t\t\tbatch_x, batch_y = self.data_handler.getBatch(batch_idx, batch_size, params['noise_level'], params['denoising'] )\n\t\t\t\tbatch_x = torch.from_numpy(batch_x).type(dtype)\n\t\t\t\tbatch_y = torch.from_numpy(batch_y).type(dtype)\n\t\t\t\tout, h, loss, loss_terms = self.model(batch_x, batch_y)\n\t\t\t\treconstruction_loss, asl_loss = loss_terms\n\t\t\t\tloss.backward()\n\t\t\t\toptimizer.step()\n\t\t\t\tepoch_losses[0]+=reconstruction_loss.item()\n\t\t\t\tepoch_losses[1]+=asl_loss.item()\n\t\t\t\tepoch_losses[2]+=0\n\t\t\t\tepoch_losses[3]+=loss.item()\n\t\t\tprint(\"After epoch %r, Reconstruction Loss = %.4f, ASL = %.4f,\"\\\n\t\t\t\t\t\t\"PSL = %.4f, and total = %.4f\"\n\t\t\t\t\t\t%(iteration+1, epoch_losses[0], epoch_losses[1], epoch_losses[2], epoch_losses[3]) )\n\t\t\t#logging.info(\"After epoch %r, Sparsity = %.1f\"\n\t\t\t#\t\t\t%(iteration+1, utils.compute_sparsity(h.cpu().data.numpy())))\n\t\t\t\t#break\n\t\t\t#break\n\n\tdef getSpineEmbeddings(self, batch_size, params):\n\t\tret = []\n\t\tself.data_handler.resetDataOrder()\n\t\tnum_batches = self.data_handler.getNumberOfBatches(batch_size)\n\t\tfor batch_idx in range(num_batches):\n\t\t\tbatch_x, batch_y = self.data_handler.getBatch(batch_idx, batch_size, params['noise_level'], params['denoising'] )\n\t\t\tbatch_x = torch.from_numpy(batch_x).type(self.dtype)\n\t\t\tbatch_y = torch.from_numpy(batch_y).type(self.dtype)\n\t\t\t_, h, _, _ = self.model(batch_x, batch_y)\n\t\t\tret.extend(h.cpu().data.numpy())\n\t\treturn np.array(ret)\n\n\tdef getWordsList(self):\n\t\treturn self.data_handler.getWordsList()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:07.089839Z","iopub.execute_input":"2021-06-14T15:35:07.090152Z","iopub.status.idle":"2021-06-14T15:35:07.108741Z","shell.execute_reply.started":"2021-06-14T15:35:07.090120Z","shell.execute_reply":"2021-06-14T15:35:07.107521Z"},"id":"Mo8oZ0MgvNj3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. hyperparameters and input/output(without psl loss)","metadata":{"id":"U7G7agxivNj4"}},{"cell_type":"code","source":"params = {}\nparams.update({\"hdim\": 1000})\nparams.update({\"denoising\": False})\nparams.update({\"noise_level\": 0.2})\nparams.update({\"num_epochs\": 100})\nparams.update({\"batch_size\": 64})\nparams.update({\"sparsity\": 0.85})\n# I use Colab here so I simply copy the data's path, I'll upload all the data this notebook need, if the path doesn't work, please change it manualy.\nparams.update({\"input\": \"/content/glove.6B.300d.txt\"})\nparams.update({\"output\": \"/content/glove.6B.300d.txt_without_psl.spine\"})\n\n\n#parser = argparse.ArgumentParser(\n#    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n'''\nparser.add_argument('--hdim', dest='hdim', type=int, default=1000,\n                    help='resultant embedding size')\n\nparser.add_argument('--denoising', dest='denoising',\n\t\t\t\t\tdefault=False,\n\t\t\t\t\taction='store_true',\n                    help='noise amount for denoising auto-encoder')\n\nparser.add_argument('--noise', dest='noise_level', type=float,\n\t\t\t\t\tdefault=0.2,\n                    help='noise amount for denoising auto-encoder')\n\nparser.add_argument('--num_epochs', dest='num_epochs', type=int,\n\t\t\t\t\tdefault=100,\n                    help='number of epochs')\n\nparser.add_argument('--batch_size', dest='batch_size', type=int,\n\t\t\t\t\tdefault=64,\n                    help='batch size')\n\nparser.add_argument('--sparsity', dest='sparsity', type=float,\n\t\t\t\t\tdefault=0.85,\n                    help='sparsity')\n\nparser.add_argument('--input', dest='input',\n\t\t\t\t\tdefault = \"../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\" ,\n                    help='input src')\n\nparser.add_argument('--output', dest='output',\n\t\t\t\t\tdefault = \"./output/glove.6B.200d.txt.spine\" ,\n                    help='output')\n'''","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:44.386555Z","iopub.execute_input":"2021-06-14T15:35:44.386978Z","iopub.status.idle":"2021-06-14T15:35:44.398138Z","shell.execute_reply.started":"2021-06-14T15:35:44.386943Z","shell.execute_reply":"2021-06-14T15:35:44.396714Z"},"id":"dljPDjtCvNj4","outputId":"d1b2c232-ef8c-44fc-c7d0-fb916143516b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.info(\"PARAMS = \" + str(params))\nlogging.info(\"=\"*41)\nsolver = Solver(params)\nsolver.train(params)\n\n# dumping the final vectors\nlogging.info(\"Dumping the final SPine embeddings\")\noutput_path = params['output'] #+ \".spine\"\nfinal_batch_size = 512\nspine_embeddings = solver.getSpineEmbeddings(final_batch_size, params)\ndump_vectors(spine_embeddings, output_path, solver.getWordsList())","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:48.324222Z","iopub.execute_input":"2021-06-14T15:35:48.324842Z","iopub.status.idle":"2021-06-14T15:35:55.120288Z","shell.execute_reply.started":"2021-06-14T15:35:48.324769Z","shell.execute_reply":"2021-06-14T15:35:55.119135Z"},"id":"tZ-awUGpvNj6","outputId":"da164cf2-b4df-40b3-f422-fec007889231","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"establisch model(without asl loss)","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO)\n\nclass SPINEModel(torch.nn.Module):\n    def __init__(self, params):\n        super(SPINEModel, self).__init__()\n        # params\n        self.inp_dim = params['inp_dim']\n        self.hdim = params['hdim']\n        self.noise_level = params['noise_level']\n        self.getReconstructionLoss = nn.MSELoss()\n        self.rho_star = 1.0 - params['sparsity']\n        \n        # autoencoder\n        logging.info(\"Building model \")\n        self.linear1 = nn.Linear(self.inp_dim, self.hdim)\n        self.linear2 = nn.Linear(self.hdim, self.inp_dim)\n        \n\n    def forward(self, batch_x, batch_y):\n        # forward\n        batch_size = batch_x.size(0)\n        linear1_out = self.linear1(batch_x)\n        h = linear1_out.clamp(min=0, max=1) # capped relu\n        out = self.linear2(h)\n\n        # different terms of the loss\n        reconstruction_loss = self.getReconstructionLoss(out, batch_y) # reconstruction loss\n        psl_loss = self._getPSLLoss(h, batch_size) \t\t# partial sparsity loss\n        #asl_loss = self._getASLLoss(h)    \t# average sparsity loss\n        total_loss = reconstruction_loss + psl_loss\n        \n        return out, h, total_loss, [reconstruction_loss, psl_loss]\n\n\n    def _getPSLLoss(self,h, batch_size):\n        return torch.sum(h*(1-h))/ (batch_size * self.hdim)\n\n\n    def _getASLLoss(self, h):\n        temp = torch.mean(h, dim=0) - self.rho_star\n        temp = temp.clamp(min=0)\n        return torch.sum(temp * temp) / self.hdim","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:07.023756Z","iopub.execute_input":"2021-06-14T15:35:07.024227Z","iopub.status.idle":"2021-06-14T15:35:07.055622Z","shell.execute_reply.started":"2021-06-14T15:35:07.024184Z","shell.execute_reply":"2021-06-14T15:35:07.054687Z"},"id":"KocJnAayvNjz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO)\n\nclass Solver:\n\n\tdef __init__(self, params):\n\n\t\t# Build data handler\n\t\tself.data_handler = DataHandler()\n\t\tself.data_handler.loadData(params['input'])\n\t\tparams['inp_dim'] = self.data_handler.getDataShape()[1]\n\t\tlogging.info(\"=\"*41)\n\n\n\t\t# Build model\n\t\tself.model = SPINEModel(params)\n\t\tself.dtype = torch.FloatTensor\n\t\tuse_cuda = torch.cuda.is_available()\n\t\tif use_cuda:\n\t\t\tself.model.cuda()\n\t\t\tself.dtype = torch.cuda.FloatTensor\n\t\tself.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.1)\n\t\tlogging.info(\"=\"*41)\n\n\n\tdef train(self, params):\n\t\tnum_epochs, batch_size = params['num_epochs'], params['batch_size'],\n\t\toptimizer = self.optimizer\n\t\tdtype = self.dtype\n\t\tfor iteration in range(num_epochs):\n\t\t\tself.data_handler.shuffleTrain()\n\t\t\tnum_batches = self.data_handler.getNumberOfBatches(batch_size)\n\t\t\tepoch_losses = np.zeros(4) # rl, asl, psl, total\n\t\t\tfor batch_idx in range(num_batches):\n\t\t\t\toptimizer.zero_grad()\n\t\t\t\tbatch_x, batch_y = self.data_handler.getBatch(batch_idx, batch_size, params['noise_level'], params['denoising'] )\n\t\t\t\tbatch_x = torch.from_numpy(batch_x).type(dtype)\n\t\t\t\tbatch_y = torch.from_numpy(batch_y).type(dtype)\n\t\t\t\tout, h, loss, loss_terms = self.model(batch_x, batch_y)\n\t\t\t\treconstruction_loss, psl_loss = loss_terms\n\t\t\t\tloss.backward()\n\t\t\t\toptimizer.step()\n\t\t\t\tepoch_losses[0]+=reconstruction_loss.item()\n\t\t\t\tepoch_losses[1]+=0\n\t\t\t\tepoch_losses[2]+=psl_loss.item()\n\t\t\t\tepoch_losses[3]+=loss.item()\n\t\t\tprint(\"After epoch %r, Reconstruction Loss = %.4f, ASL = %.4f,\"\\\n\t\t\t\t\t\t\"PSL = %.4f, and total = %.4f\"\n\t\t\t\t\t\t%(iteration+1, epoch_losses[0], epoch_losses[1], epoch_losses[2], epoch_losses[3]) )\n\t\t\t#logging.info(\"After epoch %r, Sparsity = %.1f\"\n\t\t\t#\t\t\t%(iteration+1, utils.compute_sparsity(h.cpu().data.numpy())))\n\t\t\t\t#break\n\t\t\t#break\n\n\tdef getSpineEmbeddings(self, batch_size, params):\n\t\tret = []\n\t\tself.data_handler.resetDataOrder()\n\t\tnum_batches = self.data_handler.getNumberOfBatches(batch_size)\n\t\tfor batch_idx in range(num_batches):\n\t\t\tbatch_x, batch_y = self.data_handler.getBatch(batch_idx, batch_size, params['noise_level'], params['denoising'] )\n\t\t\tbatch_x = torch.from_numpy(batch_x).type(self.dtype)\n\t\t\tbatch_y = torch.from_numpy(batch_y).type(self.dtype)\n\t\t\t_, h, _, _ = self.model(batch_x, batch_y)\n\t\t\tret.extend(h.cpu().data.numpy())\n\t\treturn np.array(ret)\n\n\tdef getWordsList(self):\n\t\treturn self.data_handler.getWordsList()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:07.089839Z","iopub.execute_input":"2021-06-14T15:35:07.090152Z","iopub.status.idle":"2021-06-14T15:35:07.108741Z","shell.execute_reply.started":"2021-06-14T15:35:07.090120Z","shell.execute_reply":"2021-06-14T15:35:07.107521Z"},"id":"Mo8oZ0MgvNj3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {}\nparams.update({\"hdim\": 1000})\nparams.update({\"denoising\": False})\nparams.update({\"noise_level\": 0.2})\nparams.update({\"num_epochs\": 100})\nparams.update({\"batch_size\": 64})\nparams.update({\"sparsity\": 0.85})\n# I use Colab here so I simply copy the data's path, I'll upload all the data this notebook need, if the path doesn't work, please change it manualy.\nparams.update({\"input\": \"/content/glove.6B.300d.txt\"})\nparams.update({\"output\": \"/content/glove.6B.300d.txt_without_asl.spine\"})\n\n\n#parser = argparse.ArgumentParser(\n#    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n'''\nparser.add_argument('--hdim', dest='hdim', type=int, default=1000,\n                    help='resultant embedding size')\n\nparser.add_argument('--denoising', dest='denoising',\n\t\t\t\t\tdefault=False,\n\t\t\t\t\taction='store_true',\n                    help='noise amount for denoising auto-encoder')\n\nparser.add_argument('--noise', dest='noise_level', type=float,\n\t\t\t\t\tdefault=0.2,\n                    help='noise amount for denoising auto-encoder')\n\nparser.add_argument('--num_epochs', dest='num_epochs', type=int,\n\t\t\t\t\tdefault=100,\n                    help='number of epochs')\n\nparser.add_argument('--batch_size', dest='batch_size', type=int,\n\t\t\t\t\tdefault=64,\n                    help='batch size')\n\nparser.add_argument('--sparsity', dest='sparsity', type=float,\n\t\t\t\t\tdefault=0.85,\n                    help='sparsity')\n\nparser.add_argument('--input', dest='input',\n\t\t\t\t\tdefault = \"../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\" ,\n                    help='input src')\n\nparser.add_argument('--output', dest='output',\n\t\t\t\t\tdefault = \"./output/glove.6B.200d.txt.spine\" ,\n                    help='output')\n'''","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:44.386555Z","iopub.execute_input":"2021-06-14T15:35:44.386978Z","iopub.status.idle":"2021-06-14T15:35:44.398138Z","shell.execute_reply.started":"2021-06-14T15:35:44.386943Z","shell.execute_reply":"2021-06-14T15:35:44.396714Z"},"id":"dljPDjtCvNj4","outputId":"d1b2c232-ef8c-44fc-c7d0-fb916143516b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.info(\"PARAMS = \" + str(params))\nlogging.info(\"=\"*41)\nsolver = Solver(params)\nsolver.train(params)\n\n# dumping the final vectors\nlogging.info(\"Dumping the final SPine embeddings\")\noutput_path = params['output'] #+ \".spine\"\nfinal_batch_size = 512\nspine_embeddings = solver.getSpineEmbeddings(final_batch_size, params)\ndump_vectors(spine_embeddings, output_path, solver.getWordsList())","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:48.324222Z","iopub.execute_input":"2021-06-14T15:35:48.324842Z","iopub.status.idle":"2021-06-14T15:35:55.120288Z","shell.execute_reply.started":"2021-06-14T15:35:48.324769Z","shell.execute_reply":"2021-06-14T15:35:55.119135Z"},"id":"tZ-awUGpvNj6","outputId":"da164cf2-b4df-40b3-f422-fec007889231","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}